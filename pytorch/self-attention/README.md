# Transformers and Self Attention

Custom implementation of transformers and multihead self attention. I implemented as I understand it and not sure if actually correct.

Both custom transformer and multihead self attention can be imported as modules for training.

## Todo

- Use in training to verify if actually works.

## References

- Transformers, Self Attention paper, https://arxiv.org/abs/1706.03762.
